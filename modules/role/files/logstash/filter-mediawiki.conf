# vim:set sw=2 ts=2 sts=2 et
# Process MediaWiki log output sent as Monolog's Logstash format
# From:
# https://github.com/wikimedia/operations-puppet/blob/e959321aa620b77403cc9379db2e86080323c6e8/files/logstash/filter-mediawiki.conf
# https://github.com/wikimedia/mediawiki-vagrant/blob/b072a101d106dd904546bda356f143c2a2bce000/puppet/modules/role/files/elk/filter-syslog.conf
filter {

  if [type] == "mediawiki" {
    # Tag for storage in elasticsearch
    mutate {
      add_tag => [ "es" ]
    }

    if [channel] == "api" {
      # Excluded because the message volume is too high
      drop {}
    }

    if [channel] == "exception" {
      # Excluded because events are duplicated by exception-json
      drop {}
    }

    if [channel] == "exception-json" {
      # Parse message as json and put elements in event
      json {
        source => "message"
      }
      mutate {
        # Rename the `id` field to `exeception_id`
        rename => [ "id", "exception_id" ]
        # Add a `class` field with the exception class name
        # Add a field to compute a checksum value based on message + file + line
        add_field => [
          "class", "%{type}",
          "message_checksum", "%{message}|%{file}|%{line}"
        ]
        # Rename the type back to "mediawiki"
        # Rename the channel to "exception"
        replace => [
          "type", "mediawiki",
          "channel", "exception"
        ]
        add_tag => [ "exception-json" ]
      }
      # Convert message_checksum field to md5 hash
      anonymize {
        fields => [ "message_checksum" ]
        algorithm => "MD5"
        key => "boringsalt"
      }
    } # end [channel] == "exception-json"

    if [channel] == "error-json" {
        # Parse message as json and put elements in event
        json {
          source => "message"
        }
        mutate {
          # Rename the `id` field to `error_id`
          rename => [ "id", "error_id" ]
          # Add a `class` field with the exception class name
          # Add a field to compute a checksum value based on message+file+line
          add_field => [
            "class", "%{type}",
            "message_checksum", "%{message}|%{file}|%{line}"
          ]
          # Rename the type back to "mediawiki"
          replace => [
            "type", "mediawiki"
          ]
        }
      } # end [channel] == "error-json"

    if [channel] == "api-feature-usage" {
      grok {
        match => [
          "message",
          "^(?m)%{QS:feature} %{QS:username} %{QS:ip} %{QS:referer} %{QS:agent}$"
        ]
        named_captures_only => true
      }

      if !("_grokparsefailure" in [tags]) {
        # Unquote ('"foo \"bar\""' to 'foo "bar"')
        mutate {
          # Strip outer quotes
          gsub => [
              "feature",  '^"|"$', "",
              "username", '^"|"$', "",
              "ip",       '^"|"$', "",
              "referer",  '^"|"$', "",
              "agent",    '^"|"$', ""
          ]
        }
        mutate {
          # Strip backslash escape characters
          gsub => [
              "feature",  '\\(.)', '\1',
              "username", '\\(.)', '\1',
              "ip",       '\\(.)', '\1',
              "referer",  '\\(.)', '\1',
              "agent",    '\\(.)', '\1'
          ]
        }

        mutate {
          replace => [ "message", "%{feature}" ]
        }

        urldecode {
          field => "username"
        }

        useragent {
          source => "agent"
          prefix => "ua_"
        }
      }
    } # end [channel] == "api-feature-usage"

    if [channel] == "xff" {
      # Copy XFF addresses from message
      grok {
        match => [
          "message",
          "^%{URI:url}\t(?:, )?(?<xff>(?:%{IP}(?:, )?)+)\t"
        ]
        named_captures_only => true
      }
      # Turn comma separated list of XFF addresses into a real list
      mutate {
        split => [ "xff", ", " ]
      }
    } # end [channel] == "xff"

  } # end [type] == "mediawiki"

}
